Net Architecture:
Resnet18Skip(
  (res18_backbone): Sequential(
    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  )
  (conv2_x): Sequential(
    (0): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): BasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (conv3_x): Sequential(
    (0): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (conv4_x): Sequential(
    (0): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (conv5_x): Sequential(
    (0): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (top_conv): Sequential(
    (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
    (1): ReLU()
  )
  (lateral_conv1): Sequential(
    (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
    (1): ReLU()
  )
  (lateral_conv2): Sequential(
    (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))
    (1): ReLU()
  )
  (lateral_conv3): Sequential(
    (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))
    (1): ReLU()
  )
  (segmentation_conv): Sequential(
    (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU()
    (2): Conv2d(64, 6, kernel_size=(1, 1), stride=(1, 1))
  )
  (criterion): CrossEntropyLoss()
)
Loss Function: CrossEntropyLoss

===========================================================
==================== Hyper-parameters =====================
n_classes: 5
lr: 0.05
epochs: 40
batch_size: 64
weight_decay: 0.0001
scheduler_step: 5
scheduler_gamma: 0.5
model_dir: model
load_best: 0
log_freq: 20
dataset_dir: dataset
===========================================================
============= Epoch 0 | 2023-09-25 18:48:01 ===============
=> Current Lr: 0.05
[0/52]: 1.9170
[20/52]: 0.3499
[40/52]: 0.2391
=> Training Loss: 6.3504, Evaluation Loss 0.2502

============= Epoch 1 | 2023-09-25 18:48:47 ===============
=> Current Lr: 0.05
[0/52]: 0.2169
[20/52]: 0.1853
[40/52]: 0.2032
=> Training Loss: 0.2037, Evaluation Loss 0.1760

============= Epoch 2 | 2023-09-25 18:49:22 ===============
=> Current Lr: 0.05
[0/52]: 0.1939
[20/52]: 0.1562
[40/52]: 0.1349
=> Training Loss: 0.1495, Evaluation Loss 0.1177

============= Epoch 3 | 2023-09-25 18:49:58 ===============
=> Current Lr: 0.05
[0/52]: 0.1244
[20/52]: 1.0735
[40/52]: 0.1693
=> Training Loss: 0.1928, Evaluation Loss 0.1546

============= Epoch 4 | 2023-09-25 18:50:34 ===============
=> Current Lr: 0.05
[0/52]: 0.1598
[20/52]: 0.1258
[40/52]: 0.1422
=> Training Loss: 0.1479, Evaluation Loss 0.1224

============= Epoch 5 | 2023-09-25 18:51:10 ===============
=> Current Lr: 0.025
[0/52]: 0.1084
[20/52]: 0.1231
[40/52]: 0.1334
=> Training Loss: 0.1220, Evaluation Loss 0.1088

============= Epoch 6 | 2023-09-25 18:51:46 ===============
=> Current Lr: 0.025
[0/52]: 0.1258
[20/52]: 0.1138
[40/52]: 0.1186
=> Training Loss: 0.1115, Evaluation Loss 0.1009

============= Epoch 7 | 2023-09-25 18:52:23 ===============
=> Current Lr: 0.025
[0/52]: 0.0993
[20/52]: 0.1308
[40/52]: 0.0709
=> Training Loss: 0.1031, Evaluation Loss 0.1020

============= Epoch 8 | 2023-09-25 18:52:59 ===============
=> Current Lr: 0.025
[0/52]: 0.0952
[20/52]: 0.0970
[40/52]: 0.0952
=> Training Loss: 0.0994, Evaluation Loss 0.0855

============= Epoch 9 | 2023-09-25 18:53:37 ===============
=> Current Lr: 0.025
[0/52]: 0.0853
[20/52]: 0.0883
[40/52]: 0.0956
=> Training Loss: 0.0910, Evaluation Loss 0.0850

============= Epoch 10 | 2023-09-25 18:54:15 ==============
=> Current Lr: 0.0125
[0/52]: 0.0676
[20/52]: 0.0707
[40/52]: 0.0712
=> Training Loss: 0.0839, Evaluation Loss 0.0850

============= Epoch 11 | 2023-09-25 18:54:51 ==============
=> Current Lr: 0.0125
[0/52]: 0.0667
[20/52]: 0.0704
[40/52]: 0.0872
=> Training Loss: 0.0816, Evaluation Loss 0.0749

============= Epoch 12 | 2023-09-25 18:55:27 ==============
=> Current Lr: 0.0125
[0/52]: 0.0911
[20/52]: 0.0765
[40/52]: 0.0793
=> Training Loss: 0.0820, Evaluation Loss 0.0738

============= Epoch 13 | 2023-09-25 18:56:03 ==============
=> Current Lr: 0.0125
[0/52]: 0.0934
[20/52]: 0.0729
[40/52]: 0.0703
=> Training Loss: 0.0798, Evaluation Loss 0.0684

============= Epoch 14 | 2023-09-25 18:56:43 ==============
=> Current Lr: 0.0125
[0/52]: 0.0685
[20/52]: 0.1114
[40/52]: 0.0765
=> Training Loss: 0.0770, Evaluation Loss 0.0696

============= Epoch 15 | 2023-09-25 18:57:20 ==============
=> Current Lr: 0.00625
[0/52]: 0.0727
[20/52]: 0.0656
[40/52]: 0.0729
=> Training Loss: 0.0735, Evaluation Loss 0.0677

============= Epoch 16 | 2023-09-25 18:57:55 ==============
=> Current Lr: 0.00625
[0/52]: 0.0731
[20/52]: 0.0659
[40/52]: 0.0743
=> Training Loss: 0.0712, Evaluation Loss 0.0657

============= Epoch 17 | 2023-09-25 18:58:31 ==============
=> Current Lr: 0.00625
[0/52]: 0.0671
[20/52]: 0.0676
[40/52]: 0.0723
=> Training Loss: 0.0717, Evaluation Loss 0.0663

============= Epoch 18 | 2023-09-25 18:59:07 ==============
=> Current Lr: 0.00625
[0/52]: 0.0752
[20/52]: 0.0781
[40/52]: 0.0600
=> Training Loss: 0.0712, Evaluation Loss 0.0676

============= Epoch 19 | 2023-09-25 18:59:43 ==============
=> Current Lr: 0.00625
[0/52]: 0.0658
[20/52]: 0.0703
[40/52]: 0.0586
=> Training Loss: 0.0694, Evaluation Loss 0.0611

============= Epoch 20 | 2023-09-25 19:00:19 ==============
=> Current Lr: 0.003125
[0/52]: 0.0760
[20/52]: 0.0526
[40/52]: 0.0801
=> Training Loss: 0.0667, Evaluation Loss 0.0631

============= Epoch 21 | 2023-09-25 19:00:55 ==============
=> Current Lr: 0.003125
[0/52]: 0.0612
[20/52]: 0.0730
[40/52]: 0.0569
=> Training Loss: 0.0647, Evaluation Loss 0.0588

============= Epoch 22 | 2023-09-25 19:01:31 ==============
=> Current Lr: 0.003125
[0/52]: 0.0614
[20/52]: 0.0529
[40/52]: 0.0800
=> Training Loss: 0.0660, Evaluation Loss 0.0625

============= Epoch 23 | 2023-09-25 19:02:08 ==============
=> Current Lr: 0.003125
[0/52]: 0.0801
[20/52]: 0.0561
[40/52]: 0.0544
=> Training Loss: 0.0675, Evaluation Loss 0.0589

============= Epoch 24 | 2023-09-25 19:02:46 ==============
=> Current Lr: 0.003125
[0/52]: 0.0648
[20/52]: 0.0712
[40/52]: 0.0687
=> Training Loss: 0.0646, Evaluation Loss 0.0572

============= Epoch 25 | 2023-09-25 19:03:22 ==============
=> Current Lr: 0.0015625
[0/52]: 0.0615
[20/52]: 0.0828
[40/52]: 0.0722
=> Training Loss: 0.0630, Evaluation Loss 0.0575

============= Epoch 26 | 2023-09-25 19:03:58 ==============
=> Current Lr: 0.0015625
[0/52]: 0.0553
[20/52]: 0.0639
[40/52]: 0.0601
=> Training Loss: 0.0610, Evaluation Loss 0.0580

============= Epoch 27 | 2023-09-25 19:04:34 ==============
=> Current Lr: 0.0015625
[0/52]: 0.0559
[20/52]: 0.0621
[40/52]: 0.0601
=> Training Loss: 0.0626, Evaluation Loss 0.0525

============= Epoch 28 | 2023-09-25 19:05:10 ==============
=> Current Lr: 0.0015625
[0/52]: 0.0673
[20/52]: 0.0627
[40/52]: 0.0567
=> Training Loss: 0.0611, Evaluation Loss 0.0570

============= Epoch 29 | 2023-09-25 19:05:45 ==============
=> Current Lr: 0.0015625
[0/52]: 0.0503
[20/52]: 0.0548
[40/52]: 0.0489
=> Training Loss: 0.0603, Evaluation Loss 0.0539

============= Epoch 30 | 2023-09-25 19:06:23 ==============
=> Current Lr: 0.00078125
[0/52]: 0.0643
[20/52]: 0.0650
[40/52]: 0.0575
=> Training Loss: 0.0595, Evaluation Loss 0.0562

============= Epoch 31 | 2023-09-25 19:07:01 ==============
=> Current Lr: 0.00078125
[0/52]: 0.0596
[20/52]: 0.0544
[40/52]: 0.0588
=> Training Loss: 0.0588, Evaluation Loss 0.0547

============= Epoch 32 | 2023-09-25 19:07:40 ==============
=> Current Lr: 0.00078125
[0/52]: 0.0611
[20/52]: 0.0625
[40/52]: 0.0570
=> Training Loss: 0.0588, Evaluation Loss 0.0558

============= Epoch 33 | 2023-09-25 19:08:20 ==============
=> Current Lr: 0.00078125
[0/52]: 0.0533
[20/52]: 0.0612
[40/52]: 0.0482
=> Training Loss: 0.0581, Evaluation Loss 0.0533

============= Epoch 34 | 2023-09-25 19:08:58 ==============
=> Current Lr: 0.00078125
[0/52]: 0.0464
[20/52]: 0.0672
[40/52]: 0.0572
=> Training Loss: 0.0575, Evaluation Loss 0.0516

============= Epoch 35 | 2023-09-25 19:09:34 ==============
=> Current Lr: 0.000390625
[0/52]: 0.0629
[20/52]: 0.0597
[40/52]: 0.0539
=> Training Loss: 0.0577, Evaluation Loss 0.0508

============= Epoch 36 | 2023-09-25 19:10:11 ==============
=> Current Lr: 0.000390625
[0/52]: 0.0559
[20/52]: 0.0497
[40/52]: 0.0582
=> Training Loss: 0.0559, Evaluation Loss 0.0491

============= Epoch 37 | 2023-09-25 19:10:48 ==============
=> Current Lr: 0.000390625
[0/52]: 0.0551
[20/52]: 0.0548
[40/52]: 0.0498
=> Training Loss: 0.0549, Evaluation Loss 0.0502

============= Epoch 38 | 2023-09-25 19:11:28 ==============
=> Current Lr: 0.000390625
[0/52]: 0.0578
[20/52]: 0.0534
[40/52]: 0.0500
=> Training Loss: 0.0555, Evaluation Loss 0.0512

============= Epoch 39 | 2023-09-25 19:12:06 ==============
=> Current Lr: 0.000390625
[0/52]: 0.0561
[20/52]: 0.0619
[40/52]: 0.0609
=> Training Loss: 0.0553, Evaluation Loss 0.0526
