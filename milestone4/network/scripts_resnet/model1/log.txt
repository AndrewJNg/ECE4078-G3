Net Architecture:
Resnet18Skip(
  (res18_backbone): Sequential(
    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  )
  (conv2_x): Sequential(
    (0): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): BasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (conv3_x): Sequential(
    (0): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (conv4_x): Sequential(
    (0): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (conv5_x): Sequential(
    (0): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (top_conv): Sequential(
    (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
    (1): ReLU()
  )
  (lateral_conv1): Sequential(
    (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
    (1): ReLU()
  )
  (lateral_conv2): Sequential(
    (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))
    (1): ReLU()
  )
  (lateral_conv3): Sequential(
    (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))
    (1): ReLU()
  )
  (segmentation_conv): Sequential(
    (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU()
    (2): Conv2d(64, 6, kernel_size=(1, 1), stride=(1, 1))
  )
  (criterion): CrossEntropyLoss()
)
Loss Function: CrossEntropyLoss

===========================================================
==================== Hyper-parameters =====================
n_classes: 5
lr: 0.05
epochs: 200
batch_size: 128
weight_decay: 0.0001
scheduler_step: 5
scheduler_gamma: 0.5
model_dir: model
load_best: 0
log_freq: 20
dataset_dir: dataset
===========================================================
============= Epoch 0 | 2023-09-15 01:11:49 ===============
=> Current Lr: 0.05
[0/115]: 1.4914
[20/115]: 0.6117
[40/115]: 0.5387
[60/115]: 0.5881
[80/115]: 0.5364
[100/115]: 0.5392
=> Training Loss: 7.0026, Evaluation Loss 0.5663

============= Epoch 1 | 2023-09-15 01:21:55 ===============
=> Current Lr: 0.05
[0/115]: 0.5202
[20/115]: 0.5932
[40/115]: 0.6302
[60/115]: 0.5410
[80/115]: 0.5799
[100/115]: 0.5907
=> Training Loss: 0.6336, Evaluation Loss 0.5664

============= Epoch 2 | 2023-09-15 01:31:42 ===============
=> Current Lr: 0.05
[0/115]: 0.5665
[20/115]: 0.5599
[40/115]: 0.5781
[60/115]: 0.5391
[80/115]: 0.5030
[100/115]: 0.5223
=> Training Loss: 0.5563, Evaluation Loss 0.5665

============= Epoch 3 | 2023-09-15 01:41:27 ===============
=> Current Lr: 0.05
[0/115]: 0.5393
[20/115]: 0.5071
[40/115]: 0.5092
[60/115]: 0.5642
[80/115]: 0.5481
[100/115]: 0.5141
=> Training Loss: 0.5364, Evaluation Loss 0.4691

============= Epoch 4 | 2023-09-15 01:51:10 ===============
=> Current Lr: 0.05
[0/115]: 0.4536
[20/115]: 0.3489
[40/115]: 0.5109
[60/115]: 0.4678
[80/115]: 0.4022
[100/115]: 0.3812
=> Training Loss: 0.4138, Evaluation Loss 0.3705

============= Epoch 5 | 2023-09-15 02:00:55 ===============
=> Current Lr: 0.025
[0/115]: 0.3723
[20/115]: 0.3418
[40/115]: 0.2628
[60/115]: 0.3282
[80/115]: 0.3024
[100/115]: 0.3142
=> Training Loss: 0.3365, Evaluation Loss 0.3297

============= Epoch 6 | 2023-09-15 02:10:43 ===============
=> Current Lr: 0.025
[0/115]: 0.3525
[20/115]: 0.3251
[40/115]: 0.3301
[60/115]: 0.2793
[80/115]: 0.3117
[100/115]: 0.2664
=> Training Loss: 0.3076, Evaluation Loss 0.3411

============= Epoch 7 | 2023-09-15 02:20:28 ===============
=> Current Lr: 0.025
[0/115]: 0.3144
[20/115]: 0.2965
[40/115]: 0.2890
[60/115]: 0.2637
[80/115]: 0.2529
[100/115]: 0.2855
=> Training Loss: 0.2873, Evaluation Loss 0.2879

============= Epoch 8 | 2023-09-15 02:30:10 ===============
=> Current Lr: 0.025
[0/115]: 0.2782
[20/115]: 0.2820
[40/115]: 0.2894
[60/115]: 0.2622
[80/115]: 0.2672
[100/115]: 0.2426
=> Training Loss: 0.2761, Evaluation Loss 0.2849

============= Epoch 9 | 2023-09-15 02:39:53 ===============
=> Current Lr: 0.025
[0/115]: 0.2831
[20/115]: 0.3042
[40/115]: 0.2520
[60/115]: 0.2772
[80/115]: 0.2893
[100/115]: 0.2398
=> Training Loss: 0.2710, Evaluation Loss 0.2683

============= Epoch 10 | 2023-09-15 02:49:34 ==============
=> Current Lr: 0.0125
[0/115]: 0.2679
[20/115]: 0.2499
[40/115]: 0.2664
[60/115]: 0.2571
[80/115]: 0.2293
[100/115]: 0.2902
=> Training Loss: 0.2564, Evaluation Loss 0.2574

============= Epoch 11 | 2023-09-15 02:59:17 ==============
=> Current Lr: 0.0125
[0/115]: 0.2827
[20/115]: 0.2390
[40/115]: 0.2471
[60/115]: 0.2330
[80/115]: 0.2171
[100/115]: 0.2188
=> Training Loss: 0.2469, Evaluation Loss 0.2355

============= Epoch 12 | 2023-09-15 03:08:59 ==============
=> Current Lr: 0.0125
[0/115]: 0.2082
[20/115]: 0.1953
[40/115]: 0.1680
[60/115]: 0.1821
[80/115]: 0.1741
[100/115]: 0.1864
=> Training Loss: 0.1970, Evaluation Loss 0.1793

============= Epoch 13 | 2023-09-15 03:18:42 ==============
=> Current Lr: 0.0125
[0/115]: 0.1468
[20/115]: 0.1620
[40/115]: 0.1676
[60/115]: 0.1602
[80/115]: 0.1727
[100/115]: 0.1714
=> Training Loss: 0.1762, Evaluation Loss 0.1687

============= Epoch 14 | 2023-09-15 03:28:26 ==============
=> Current Lr: 0.0125
[0/115]: 0.1648
[20/115]: 0.1601
[40/115]: 0.1493
[60/115]: 0.2018
[80/115]: 0.1560
[100/115]: 0.1591
=> Training Loss: 0.1693, Evaluation Loss 0.1643

============= Epoch 15 | 2023-09-15 03:38:09 ==============
=> Current Lr: 0.00625
[0/115]: 0.1473
[20/115]: 0.1516
[40/115]: 0.1634
[60/115]: 0.1496
[80/115]: 0.1441
[100/115]: 0.1386
=> Training Loss: 0.1557, Evaluation Loss 0.1524

============= Epoch 16 | 2023-09-15 03:47:51 ==============
=> Current Lr: 0.00625
[0/115]: 0.1479
[20/115]: 0.1446
[40/115]: 0.1446
[60/115]: 0.1544
[80/115]: 0.1534
[100/115]: 0.1377
=> Training Loss: 0.1507, Evaluation Loss 0.1509

============= Epoch 17 | 2023-09-15 03:57:33 ==============
=> Current Lr: 0.00625
[0/115]: 0.1428
[20/115]: 0.1473
[40/115]: 0.1679
[60/115]: 0.1611
[80/115]: 0.1568
[100/115]: 0.1461
=> Training Loss: 0.1479, Evaluation Loss 0.1502

============= Epoch 18 | 2023-09-15 04:07:16 ==============
=> Current Lr: 0.00625
[0/115]: 0.1259
[20/115]: 0.1495
[40/115]: 0.1266
[60/115]: 0.1861
[80/115]: 0.1448
[100/115]: 0.1721
=> Training Loss: 0.1458, Evaluation Loss 0.1448

============= Epoch 19 | 2023-09-15 04:16:58 ==============
=> Current Lr: 0.00625
[0/115]: 0.1456
[20/115]: 0.1549
[40/115]: 0.1569
[60/115]: 0.1459
[80/115]: 0.1376
[100/115]: 0.1356
=> Training Loss: 0.1426, Evaluation Loss 0.1389

============= Epoch 20 | 2023-09-15 04:26:48 ==============
=> Current Lr: 0.003125
[0/115]: 0.1270
[20/115]: 0.1265
[40/115]: 0.1277
[60/115]: 0.1261
[80/115]: 0.1239
[100/115]: 0.1610
=> Training Loss: 0.1328, Evaluation Loss 0.1267

============= Epoch 21 | 2023-09-15 04:36:30 ==============
=> Current Lr: 0.003125
[0/115]: 0.1308
[20/115]: 0.1403
[40/115]: 0.1231
[60/115]: 0.1315
[80/115]: 0.1091
[100/115]: 0.1331
=> Training Loss: 0.1266, Evaluation Loss 0.1328

============= Epoch 22 | 2023-09-15 04:46:13 ==============
=> Current Lr: 0.003125
[0/115]: 0.1645
[20/115]: 0.1405
[40/115]: 0.1215
[60/115]: 0.1053
[80/115]: 0.1142
[100/115]: 0.1218
=> Training Loss: 0.1233, Evaluation Loss 0.1164

============= Epoch 23 | 2023-09-15 04:55:53 ==============
=> Current Lr: 0.003125
[0/115]: 0.1170
[20/115]: 0.1042
[40/115]: 0.1074
[60/115]: 0.1275
[80/115]: 0.1321
[100/115]: 0.1125
=> Training Loss: 0.1205, Evaluation Loss 0.1192

============= Epoch 24 | 2023-09-15 05:05:33 ==============
=> Current Lr: 0.003125
[0/115]: 0.1179
[20/115]: 0.1201
[40/115]: 0.1058
[60/115]: 0.1107
[80/115]: 0.1277
[100/115]: 0.1147
=> Training Loss: 0.1176, Evaluation Loss 0.1179

============= Epoch 25 | 2023-09-15 05:15:13 ==============
=> Current Lr: 0.0015625
[0/115]: 0.1054
[20/115]: 0.1219
[40/115]: 0.0986
[60/115]: 0.1257
[80/115]: 0.1102
[100/115]: 0.0913
=> Training Loss: 0.1098, Evaluation Loss 0.1079

============= Epoch 26 | 2023-09-15 05:24:52 ==============
=> Current Lr: 0.0015625
[0/115]: 0.1173
[20/115]: 0.0935
[40/115]: 0.1067
[60/115]: 0.1131
[80/115]: 0.1112
[100/115]: 0.0864
=> Training Loss: 0.1078, Evaluation Loss 0.1101

============= Epoch 27 | 2023-09-15 05:34:32 ==============
=> Current Lr: 0.0015625
[0/115]: 0.1103
[20/115]: 0.1005
[40/115]: 0.1160
[60/115]: 0.1115
[80/115]: 0.1032
[100/115]: 0.0997
=> Training Loss: 0.1074, Evaluation Loss 0.1062

============= Epoch 28 | 2023-09-15 05:44:12 ==============
=> Current Lr: 0.0015625
[0/115]: 0.1068
[20/115]: 0.1066
[40/115]: 0.1079
[60/115]: 0.0914
[80/115]: 0.1043
[100/115]: 0.1114
=> Training Loss: 0.1068, Evaluation Loss 0.1038

============= Epoch 29 | 2023-09-15 05:53:53 ==============
=> Current Lr: 0.0015625
[0/115]: 0.0955
[20/115]: 0.0898
[40/115]: 0.0915
[60/115]: 0.1075
[80/115]: 0.1004
[100/115]: 0.1097
=> Training Loss: 0.1036, Evaluation Loss 0.1038

============= Epoch 30 | 2023-09-15 06:03:33 ==============
=> Current Lr: 0.00078125
[0/115]: 0.0986
[20/115]: 0.0937
[40/115]: 0.0814
[60/115]: 0.0874
[80/115]: 0.0859
[100/115]: 0.0823
=> Training Loss: 0.0997, Evaluation Loss 0.0966

============= Epoch 31 | 2023-09-15 06:13:13 ==============
=> Current Lr: 0.00078125
[0/115]: 0.1013
============= Epoch 31 | 2023-09-15 06:16:45 ==============
=> Current Lr: 0.00078125
============= Epoch 31 | 2023-09-15 06:17:31 ==============
=> Current Lr: 0.00078125
============= Epoch 31 | 2023-09-15 06:17:37 ==============
=> Current Lr: 0.00078125
[0/57]: 0.0950
[20/57]: 0.1079
============= Epoch 31 | 2023-09-15 06:31:16 ==============
=> Current Lr: 0.00078125
[0/123]: 0.0867
[20/123]: 0.0602
[40/123]: 0.0443
[60/123]: 0.0510
[80/123]: 0.0406
[100/123]: 0.0441
[120/123]: 0.0479
=> Training Loss: 0.0489, Evaluation Loss 0.0397

============= Epoch 32 | 2023-09-15 06:32:11 ==============
=> Current Lr: 0.00078125
[0/123]: 0.0415
[20/123]: 0.0458
[40/123]: 0.0316
[60/123]: 0.0373
[80/123]: 0.0321
[100/123]: 0.0338
[120/123]: 0.0353
=> Training Loss: 0.0376, Evaluation Loss 0.0346

============= Epoch 33 | 2023-09-15 06:32:59 ==============
=> Current Lr: 0.00078125
[0/123]: 0.0400
[20/123]: 0.0274
[40/123]: 0.0315
[60/123]: 0.0360
[80/123]: 0.0405
[100/123]: 0.0386
[120/123]: 0.0328
=> Training Loss: 0.0357, Evaluation Loss 0.0327

============= Epoch 34 | 2023-09-15 06:33:46 ==============
=> Current Lr: 0.00078125
[0/123]: 0.0303
[20/123]: 0.0342
[40/123]: 0.0315
[60/123]: 0.0354
[80/123]: 0.0281
[100/123]: 0.0390
[120/123]: 0.0305
=> Training Loss: 0.0351, Evaluation Loss 0.0325

============= Epoch 35 | 2023-09-15 06:34:34 ==============
=> Current Lr: 0.000390625
[0/123]: 0.0336
[20/123]: 0.0317
[40/123]: 0.0253
[60/123]: 0.0290
[80/123]: 0.0393
[100/123]: 0.0260
[120/123]: 0.0329
=> Training Loss: 0.0315, Evaluation Loss 0.0292

============= Epoch 36 | 2023-09-15 06:35:21 ==============
=> Current Lr: 0.000390625
[0/123]: 0.0274
[20/123]: 0.0274
[40/123]: 0.0283
[60/123]: 0.0297
[80/123]: 0.0373
[100/123]: 0.0337
[120/123]: 0.0290
=> Training Loss: 0.0304, Evaluation Loss 0.0282

============= Epoch 37 | 2023-09-15 06:36:09 ==============
=> Current Lr: 0.000390625
[0/123]: 0.0272
[20/123]: 0.0265
[40/123]: 0.0299
[60/123]: 0.0275
[80/123]: 0.0285
[100/123]: 0.0282
[120/123]: 0.0293
=> Training Loss: 0.0304, Evaluation Loss 0.0284

============= Epoch 38 | 2023-09-15 06:36:58 ==============
=> Current Lr: 0.000390625
[0/123]: 0.0267
[20/123]: 0.0291
[40/123]: 0.0329
[60/123]: 0.0311
[80/123]: 0.0285
[100/123]: 0.0322
[120/123]: 0.0327
=> Training Loss: 0.0292, Evaluation Loss 0.0281

============= Epoch 39 | 2023-09-15 06:37:46 ==============
=> Current Lr: 0.000390625
[0/123]: 0.0261
[20/123]: 0.0291
[40/123]: 0.0275
[60/123]: 0.0238
[80/123]: 0.0291
[100/123]: 0.0310
[120/123]: 0.0311
=> Training Loss: 0.0282, Evaluation Loss 0.0273

============= Epoch 40 | 2023-09-15 06:38:34 ==============
=> Current Lr: 0.0001953125
[0/123]: 0.0273
[20/123]: 0.0318
[40/123]: 0.0247
[60/123]: 0.0327
[80/123]: 0.0275
[100/123]: 0.0243
[120/123]: 0.0253
=> Training Loss: 0.0272, Evaluation Loss 0.0258

============= Epoch 41 | 2023-09-15 06:39:30 ==============
=> Current Lr: 0.0001953125
[0/123]: 0.0276
[20/123]: 0.0211
[40/123]: 0.0244
[60/123]: 0.0278
[80/123]: 0.0263
[100/123]: 0.0223
[120/123]: 0.0231
=> Training Loss: 0.0264, Evaluation Loss 0.0256

============= Epoch 42 | 2023-09-15 06:40:22 ==============
=> Current Lr: 0.0001953125
[0/123]: 0.0302
[20/123]: 0.0305
[40/123]: 0.0342
[60/123]: 0.0212
[80/123]: 0.0260
[100/123]: 0.0261
[120/123]: 0.0257
=> Training Loss: 0.0265, Evaluation Loss 0.0254

============= Epoch 43 | 2023-09-15 06:41:11 ==============
=> Current Lr: 0.0001953125
[0/123]: 0.0258
[20/123]: 0.0277
[40/123]: 0.0260
[60/123]: 0.0267
[80/123]: 0.0265
[100/123]: 0.0263
[120/123]: 0.0227
=> Training Loss: 0.0266, Evaluation Loss 0.0256

============= Epoch 44 | 2023-09-15 06:42:01 ==============
=> Current Lr: 0.0001953125
[0/123]: 0.0275
[20/123]: 0.0221
[40/123]: 0.0301
[60/123]: 0.0280
[80/123]: 0.0260
[100/123]: 0.0228
[120/123]: 0.0294
=> Training Loss: 0.0258, Evaluation Loss 0.0254

============= Epoch 45 | 2023-09-15 06:42:50 ==============
=> Current Lr: 9.765625e-05
[0/123]: 0.0296
[20/123]: 0.0229
[40/123]: 0.0217
[60/123]: 0.0291
[80/123]: 0.0255
[100/123]: 0.0260
[120/123]: 0.0253
=> Training Loss: 0.0253, Evaluation Loss 0.0241

============= Epoch 46 | 2023-09-15 06:43:39 ==============
=> Current Lr: 9.765625e-05
[0/123]: 0.0249
[20/123]: 0.0234
[40/123]: 0.0237
[60/123]: 0.0235
[80/123]: 0.0237
[100/123]: 0.0322
[120/123]: 0.0227
=> Training Loss: 0.0250, Evaluation Loss 0.0248

============= Epoch 47 | 2023-09-15 06:44:28 ==============
=> Current Lr: 9.765625e-05
[0/123]: 0.0253
[20/123]: 0.0245
[40/123]: 0.0262
[60/123]: 0.0232
[80/123]: 0.0259
[100/123]: 0.0281
[120/123]: 0.0272
=> Training Loss: 0.0247, Evaluation Loss 0.0244

============= Epoch 48 | 2023-09-15 06:45:17 ==============
=> Current Lr: 9.765625e-05
[0/123]: 0.0226
[20/123]: 0.0231
[40/123]: 0.0236
[60/123]: 0.0239
[80/123]: 0.0230
[100/123]: 0.0249
[120/123]: 0.0248
=> Training Loss: 0.0246, Evaluation Loss 0.0248

============= Epoch 49 | 2023-09-15 06:46:06 ==============
=> Current Lr: 9.765625e-05
[0/123]: 0.0247
[20/123]: 0.0257
[40/123]: 0.0279
[60/123]: 0.0209
[80/123]: 0.0236
[100/123]: 0.0220
[120/123]: 0.0242
=> Training Loss: 0.0248, Evaluation Loss 0.0241

============= Epoch 50 | 2023-09-15 06:46:54 ==============
=> Current Lr: 4.8828125e-05
[0/123]: 0.0228
[20/123]: 0.0281
[40/123]: 0.0277
[60/123]: 0.0229
[80/123]: 0.0234
[100/123]: 0.0199
[120/123]: 0.0259
=> Training Loss: 0.0241, Evaluation Loss 0.0236

============= Epoch 51 | 2023-09-15 06:47:43 ==============
=> Current Lr: 4.8828125e-05
[0/123]: 0.0249
[20/123]: 0.0244
[40/123]: 0.0211
[60/123]: 0.0245
[80/123]: 0.0237
[100/123]: 0.0248
[120/123]: 0.0242
=> Training Loss: 0.0240, Evaluation Loss 0.0236

============= Epoch 52 | 2023-09-15 06:48:32 ==============
=> Current Lr: 4.8828125e-05
[0/123]: 0.0243
[20/123]: 0.0255
[40/123]: 0.0249
[60/123]: 0.0247
[80/123]: 0.0240
[100/123]: 0.0303
[120/123]: 0.0252
=> Training Loss: 0.0238, Evaluation Loss 0.0232

============= Epoch 53 | 2023-09-15 06:49:21 ==============
=> Current Lr: 4.8828125e-05
[0/123]: 0.0233
[20/123]: 0.0225
[40/123]: 0.0220
[60/123]: 0.0209
[80/123]: 0.0234
[100/123]: 0.0224
[120/123]: 0.0258
=> Training Loss: 0.0239, Evaluation Loss 0.0237

============= Epoch 54 | 2023-09-15 06:50:10 ==============
=> Current Lr: 4.8828125e-05
[0/123]: 0.0210
[20/123]: 0.0216
[40/123]: 0.0231
[60/123]: 0.0207
[80/123]: 0.0231
[100/123]: 0.0232
[120/123]: 0.0258
=> Training Loss: 0.0236, Evaluation Loss 0.0230

============= Epoch 55 | 2023-09-15 06:50:58 ==============
=> Current Lr: 2.44140625e-05
[0/123]: 0.0226
[20/123]: 0.0296
[40/123]: 0.0209
[60/123]: 0.0260
[80/123]: 0.0219
[100/123]: 0.0236
[120/123]: 0.0226
=> Training Loss: 0.0236, Evaluation Loss 0.0233

============= Epoch 56 | 2023-09-15 06:51:46 ==============
=> Current Lr: 2.44140625e-05
[0/123]: 0.0261
[20/123]: 0.0215
[40/123]: 0.0245
[60/123]: 0.0189
[80/123]: 0.0247
[100/123]: 0.0224
[120/123]: 0.0199
=> Training Loss: 0.0236, Evaluation Loss 0.0228

============= Epoch 57 | 2023-09-15 06:52:35 ==============
=> Current Lr: 2.44140625e-05
[0/123]: 0.0223
[20/123]: 0.0236
[40/123]: 0.0203
[60/123]: 0.0210
[80/123]: 0.0247
[100/123]: 0.0234
[120/123]: 0.0241
=> Training Loss: 0.0233, Evaluation Loss 0.0231

============= Epoch 58 | 2023-09-15 06:53:23 ==============
=> Current Lr: 2.44140625e-05
[0/123]: 0.0245
[20/123]: 0.0222
[40/123]: 0.0303
[60/123]: 0.0309
[80/123]: 0.0225
[100/123]: 0.0216
[120/123]: 0.0234
=> Training Loss: 0.0233, Evaluation Loss 0.0233

============= Epoch 59 | 2023-09-15 06:54:11 ==============
=> Current Lr: 2.44140625e-05
[0/123]: 0.0240
[20/123]: 0.0224
[40/123]: 0.0195
[60/123]: 0.0233
[80/123]: 0.0214
[100/123]: 0.0213
[120/123]: 0.0192
=> Training Loss: 0.0231, Evaluation Loss 0.0229

============= Epoch 60 | 2023-09-15 06:55:00 ==============
=> Current Lr: 1.220703125e-05
[0/123]: 0.0183
[20/123]: 0.0220
[40/123]: 0.0256
[60/123]: 0.0197
[80/123]: 0.0208
[100/123]: 0.0188
[120/123]: 0.0173
=> Training Loss: 0.0230, Evaluation Loss 0.0227

============= Epoch 61 | 2023-09-15 06:55:49 ==============
=> Current Lr: 1.220703125e-05
[0/123]: 0.0202
[20/123]: 0.0215
[40/123]: 0.0235
[60/123]: 0.0212
[80/123]: 0.0214
[100/123]: 0.0208
[120/123]: 0.0225
=> Training Loss: 0.0232, Evaluation Loss 0.0227

============= Epoch 62 | 2023-09-15 06:56:39 ==============
=> Current Lr: 1.220703125e-05
[0/123]: 0.0253
============= Epoch 62 | 2023-09-15 07:40:01 ==============
=> Current Lr: 1.220703125e-05
[0/123]: 0.0229
[20/123]: 0.0219
[40/123]: 0.0232
[60/123]: 0.0221
[80/123]: 0.0219
[100/123]: 0.0213
[120/123]: 0.0271
=> Training Loss: 0.0229, Evaluation Loss 0.0227

============= Epoch 63 | 2023-09-15 07:40:52 ==============
=> Current Lr: 1.220703125e-05
[0/123]: 0.0240
[20/123]: 0.0250
[40/123]: 0.0242
[60/123]: 0.0267
[80/123]: 0.0226
[100/123]: 0.0244
[120/123]: 0.0232
=> Training Loss: 0.0231, Evaluation Loss 0.0226

============= Epoch 64 | 2023-09-15 07:41:37 ==============
=> Current Lr: 1.220703125e-05
[0/123]: 0.0241
[20/123]: 0.0252
[40/123]: 0.0237
[60/123]: 0.0314
[80/123]: 0.0232
[100/123]: 0.0253
[120/123]: 0.0194
=> Training Loss: 0.0230, Evaluation Loss 0.0228

============= Epoch 65 | 2023-09-15 07:42:26 ==============
=> Current Lr: 6.103515625e-06
[0/123]: 0.0232
[20/123]: 0.0227
[40/123]: 0.0218
[60/123]: 0.0235
[80/123]: 0.0218
[100/123]: 0.0223
[120/123]: 0.0227
=> Training Loss: 0.0228, Evaluation Loss 0.0227

============= Epoch 66 | 2023-09-15 07:43:27 ==============
=> Current Lr: 6.103515625e-06
[0/123]: 0.0197